"""
Physics-Constrained PINN Loss Functions

This module implements TRUE physics constraints for gravitational lensing PINNs:
1. Poisson equation: ∇²ψ = 2κ
2. Deflection gradient consistency: α = ∇ψ  
3. Proper derivative computation via torch.autograd

These constraints enforce the fundamental physics of gravitational lensing,
ensuring the network learns physically meaningful representations.

Scientific Background:
---------------------
The lensing potential ψ satisfies:
    ∇²ψ(θ) = 2κ(θ)

where κ is the convergence (dimensionless surface density).

The deflection angle is the gradient of the potential:
    α(θ) = ∇ψ(θ)

By enforcing these constraints during training, we ensure the PINN learns
the correct physical relationships, not just data fitting.

References:
----------
- Schneider, Ehlers & Falco (1992), Gravitational Lenses, Chapter 3
- Bartelmann & Schneider (2001), Phys. Rep. 340, 291
- Raissi et al. (2019), J. Comp. Phys. 378, 686 (PINN methodology)
- Lu et al. (2021), Nat. Mach. Intell. 3, 218 (Physics-constrained ML)

Author: ISEF 2025 - Task 3
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Dict, Optional
import numpy as np


class PhysicsConstrainedPINNLoss(nn.Module):
    """
    Enhanced physics-constrained loss for gravitational lensing PINNs.
    
    Implements CRITICAL physical constraints:
    1. **Poisson Equation**: ∇²ψ = 2κ
    2. **Deflection Gradient**: α = ∇ψ
    3. **Mass Conservation**: ∫κ dA = M_total
    4. **Parameter Regularization**: Physical bounds
    
    Loss Function:
    -------------
    L = L_data + λ₁·L_Poisson + λ₂·L_gradient + λ₃·L_conservation + λ₄·L_reg
    
    where:
    - L_data: MSE on parameters and classification
    - L_Poisson: ||∇²ψ - 2κ||²
    - L_gradient: ||α_pred - ∇ψ||²
    - L_conservation: Mass conservation penalty
    - L_reg: Parameter regularization
    
    Parameters
    ----------
    lambda_poisson : float
        Weight for Poisson equation constraint (default: 1.0)
    lambda_gradient : float
        Weight for gradient consistency constraint (default: 1.0)
    lambda_conservation : float
        Weight for mass conservation (default: 0.5)
    lambda_reg : float
        Weight for regularization (default: 0.01)
    lambda_classification : float
        Weight for classification loss (default: 1.0)
    use_autograd : bool
        Use torch.autograd for derivatives (default: True)
        
    Examples
    --------
    >>> loss_fn = PhysicsConstrainedPINNLoss(
    ...     lambda_poisson=1.0,
    ...     lambda_gradient=1.0,
    ...     lambda_conservation=0.5
    ... )
    >>> 
    >>> # During training:
    >>> total_loss, loss_dict = loss_fn(
    ...     psi_pred=psi,           # Lensing potential
    ...     kappa_pred=kappa,       # Convergence
    ...     alpha_pred=alpha,       # Deflection angle
    ...     params_pred=params,     # NFW parameters
    ...     params_true=params_gt,  
    ...     classes_pred=logits,
    ...     classes_true=labels
    ... )
    """
    
    def __init__(
        self,
        lambda_poisson: float = 1.0,
        lambda_gradient: float = 1.0,
        lambda_conservation: float = 0.5,
        lambda_reg: float = 0.01,
        lambda_classification: float = 1.0,
        use_autograd: bool = True
    ):
        super().__init__()
        
        self.lambda_poisson = lambda_poisson
        self.lambda_gradient = lambda_gradient
        self.lambda_conservation = lambda_conservation
        self.lambda_reg = lambda_reg
        self.lambda_classification = lambda_classification
        self.use_autograd = use_autograd
        
    def compute_laplacian_autograd(
        self,
        psi: torch.Tensor,
        grid_coords: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute ∇²ψ using torch.autograd for automatic differentiation.
        
        This is the CORRECT way to compute second derivatives in PINNs.
        
        Parameters
        ----------
        psi : torch.Tensor
            Lensing potential field [B, 1, H, W]
        grid_coords : torch.Tensor
            Coordinate grid [B, 2, H, W] where grid_coords[:, 0] = x, grid_coords[:, 1] = y
            MUST have requires_grad=True
            
        Returns
        -------
        laplacian : torch.Tensor
            ∇²ψ = ∂²ψ/∂x² + ∂²ψ/∂y²
            
        Notes
        -----
        Uses torch.autograd.grad to compute:
        1. First derivatives: ∂ψ/∂x, ∂ψ/∂y
        2. Second derivatives: ∂²ψ/∂x², ∂²ψ/∂y²
        3. Sum for Laplacian
        
        This ensures automatic differentiation through the entire computation graph.
        """
        if not grid_coords.requires_grad:
            raise ValueError("grid_coords must have requires_grad=True for autograd")
        
        B, C, H, W = psi.shape
        
        # Flatten spatial dimensions for easier gradient computation
        psi_flat = psi.view(B, -1)  # [B, H*W]
        coords_flat = grid_coords.view(B, 2, -1)  # [B, 2, H*W]
        
        # Initialize Laplacian
        laplacian_flat = torch.zeros_like(psi_flat)
        
        # Compute second derivatives for each point
        for i in range(psi_flat.shape[1]):  # For each spatial location
            # First derivatives: ∂ψ/∂x and ∂ψ/∂y
            grad_outputs = torch.ones_like(psi_flat[:, i])
            
            grad_psi = torch.autograd.grad(
                outputs=psi_flat[:, i],
                inputs=grid_coords,
                grad_outputs=grad_outputs,
                create_graph=True,  # CRITICAL: allow second derivatives
                retain_graph=True
            )[0]  # [B, 2, H, W]
            
            dpsi_dx = grad_psi[:, 0].view(B, -1)[:, i]  # ∂ψ/∂x at location i
            dpsi_dy = grad_psi[:, 1].view(B, -1)[:, i]  # ∂ψ/∂y at location i
            
            # Second derivatives: ∂²ψ/∂x²
            d2psi_dx2 = torch.autograd.grad(
                outputs=dpsi_dx,
                inputs=grid_coords,
                grad_outputs=torch.ones_like(dpsi_dx),
                create_graph=True,
                retain_graph=True
            )[0][:, 0].view(B, -1)[:, i]
            
            # ∂²ψ/∂y²
            d2psi_dy2 = torch.autograd.grad(
                outputs=dpsi_dy,
                inputs=grid_coords,
                grad_outputs=torch.ones_like(dpsi_dy),
                create_graph=True,
                retain_graph=True
            )[0][:, 1].view(B, -1)[:, i]
            
            # Laplacian = ∂²ψ/∂x² + ∂²ψ/∂y²
            laplacian_flat[:, i] = d2psi_dx2 + d2psi_dy2
        
        # Reshape back to image
        laplacian = laplacian_flat.view(B, 1, H, W)
        
        return laplacian
    
    def compute_laplacian_finite_diff(self, psi: torch.Tensor) -> torch.Tensor:
        """
        Compute ∇²ψ using finite differences (fallback method).
        
        Uses standard 5-point stencil:
            ∇²ψ ≈ [ψ(i+1,j) + ψ(i-1,j) + ψ(i,j+1) + ψ(i,j-1) - 4ψ(i,j)] / h²
        
        Parameters
        ----------
        psi : torch.Tensor
            Lensing potential [B, 1, H, W]
            
        Returns
        -------
        laplacian : torch.Tensor
            Approximate ∇²ψ
            
        Notes
        -----
        This is faster than autograd but less accurate for training.
        Use for validation and testing, not training loss.
        """
        kernel = torch.tensor([
            [0, 1, 0],
            [1, -4, 1],
            [0, 1, 0]
        ], dtype=psi.dtype, device=psi.device).view(1, 1, 3, 3)
        
        # Pad to maintain size
        psi_padded = F.pad(psi, (1, 1, 1, 1), mode='replicate')
        laplacian = F.conv2d(psi_padded, kernel, padding=0)
        
        return laplacian
    
    def compute_gradient_autograd(
        self,
        psi: torch.Tensor,
        grid_coords: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute ∇ψ = (∂ψ/∂x, ∂ψ/∂y) using torch.autograd.
        
        Parameters
        ----------
        psi : torch.Tensor
            Lensing potential [B, 1, H, W]
        grid_coords : torch.Tensor
            Coordinate grid [B, 2, H, W] with requires_grad=True
            
        Returns
        -------
        dpsi_dx : torch.Tensor
            ∂ψ/∂x [B, 1, H, W]
        dpsi_dy : torch.Tensor
            ∂ψ/∂y [B, 1, H, W]
        """
        if not grid_coords.requires_grad:
            raise ValueError("grid_coords must have requires_grad=True")
        
        # Compute gradient
        grad_psi = torch.autograd.grad(
            outputs=psi,
            inputs=grid_coords,
            grad_outputs=torch.ones_like(psi),
            create_graph=True,
            retain_graph=True
        )[0]  # [B, 2, H, W]
        
        dpsi_dx = grad_psi[:, 0:1]  # [B, 1, H, W]
        dpsi_dy = grad_psi[:, 1:2]  # [B, 1, H, W]
        
        return dpsi_dx, dpsi_dy
    
    def poisson_loss(
        self,
        psi: torch.Tensor,
        kappa: torch.Tensor,
        grid_coords: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Compute Poisson equation residual: ||∇²ψ - 2κ||².
        
        This is the FUNDAMENTAL physical constraint for lensing.
        
        Parameters
        ----------
        psi : torch.Tensor
            Predicted lensing potential [B, 1, H, W]
        kappa : torch.Tensor
            Predicted convergence [B, 1, H, W]
        grid_coords : torch.Tensor, optional
            Coordinate grid for autograd method
            
        Returns
        -------
        loss : torch.Tensor
            L_Poisson = ||∇²ψ - 2κ||²
            
        Notes
        -----
        Physical meaning: The lensing potential's curvature must equal
        twice the surface density. This is Maxwell's equation for gravity.
        """
        if self.use_autograd and grid_coords is not None:
            # Use autograd for exact derivatives
            laplacian_psi = self.compute_laplacian_autograd(psi, grid_coords)
        else:
            # Use finite differences
            laplacian_psi = self.compute_laplacian_finite_diff(psi)
        
        # Poisson equation: ∇²ψ = 2κ
        target = 2.0 * kappa
        
        # MSE loss
        loss = F.mse_loss(laplacian_psi, target)
        
        return loss
    
    def gradient_consistency_loss(
        self,
        psi: torch.Tensor,
        alpha_pred: torch.Tensor,
        grid_coords: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Compute deflection gradient consistency: ||α_pred - ∇ψ||².
        
        The deflection angle MUST be the gradient of the potential.
        
        Parameters
        ----------
        psi : torch.Tensor
            Lensing potential [B, 1, H, W]
        alpha_pred : torch.Tensor
            Predicted deflection angle [B, 2, H, W] where
            alpha_pred[:, 0] = α_x, alpha_pred[:, 1] = α_y
        grid_coords : torch.Tensor, optional
            Coordinate grid for autograd
            
        Returns
        -------
        loss : torch.Tensor
            L_gradient = ||α_pred - ∇ψ||²
            
        Notes
        -----
        This ensures the deflection field is conservative (curl-free),
        which is required by the physics of gravitational lensing.
        """
        if self.use_autograd and grid_coords is not None:
            # Use autograd
            dpsi_dx, dpsi_dy = self.compute_gradient_autograd(psi, grid_coords)
            grad_psi = torch.cat([dpsi_dx, dpsi_dy], dim=1)  # [B, 2, H, W]
        else:
            # Use finite differences
            grad_psi = self._compute_gradient_finite_diff(psi)
        
        # Consistency: α = ∇ψ
        loss = F.mse_loss(alpha_pred, grad_psi)
        
        return loss
    
    def _compute_gradient_finite_diff(
        self,
        psi: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute gradient using finite differences (fallback).
        
        Parameters
        ----------
        psi : torch.Tensor
            Potential [B, 1, H, W]
            
        Returns
        -------
        gradient : torch.Tensor
            [B, 2, H, W] where [:, 0] = ∂ψ/∂x, [:, 1] = ∂ψ/∂y
        """
        # Sobel operator for gradients
        sobel_x = torch.tensor([
            [-1, 0, 1],
            [-2, 0, 2],
            [-1, 0, 1]
        ], dtype=psi.dtype, device=psi.device).view(1, 1, 3, 3) / 8.0
        
        sobel_y = torch.tensor([
            [-1, -2, -1],
            [0, 0, 0],
            [1, 2, 1]
        ], dtype=psi.dtype, device=psi.device).view(1, 1, 3, 3) / 8.0
        
        psi_padded = F.pad(psi, (1, 1, 1, 1), mode='replicate')
        
        dpsi_dx = F.conv2d(psi_padded, sobel_x, padding=0)
        dpsi_dy = F.conv2d(psi_padded, sobel_y, padding=0)
        
        gradient = torch.cat([dpsi_dx, dpsi_dy], dim=1)
        
        return gradient
    
    def mass_conservation_loss(self, kappa: torch.Tensor) -> torch.Tensor:
        """
        Enforce mass conservation: ∫κ dA should be reasonable.
        
        Parameters
        ----------
        kappa : torch.Tensor
            Convergence map [B, 1, H, W]
            
        Returns
        -------
        loss : torch.Tensor
            Penalty for unphysical total mass
        """
        # Total mass per image
        total_mass = kappa.sum(dim=(2, 3))  # [B, 1]
        
        # Penalize negative mass (unphysical)
        negative_penalty = F.relu(-total_mass).mean()
        
        # Penalize extremely large mass (likely numerical error)
        large_penalty = F.relu(total_mass - 1000.0).mean()
        
        loss = negative_penalty + large_penalty
        
        return loss
    
    def parameter_regularization(
        self,
        params: torch.Tensor
    ) -> torch.Tensor:
        """
        Regularize parameters to stay in physical bounds.
        
        Expected parameters (NFW profile):
        0. log10(M_vir/M_sun): [10, 15]
        1. concentration c: [1, 30]
        2. redshift z: [0.1, 3.0]
        3. ellipticity e: [0, 0.8]
        4. position angle θ: [0, 180] degrees
        
        Parameters
        ----------
        params : torch.Tensor
            Predicted parameters [B, N_params]
            
        Returns
        -------
        loss : torch.Tensor
            L2 + bounds penalty
        """
        # L2 regularization (prevent extreme values)
        l2_loss = 0.01 * torch.mean(params**2)
        
        # Bounds penalties (soft constraints)
        # Assuming normalized parameters in roughly [-3, 3] range
        bounds_penalty = F.relu(torch.abs(params) - 3.0).mean()
        
        loss = l2_loss + bounds_penalty
        
        return loss
    
    def forward(
        self,
        params_pred: torch.Tensor,
        params_true: torch.Tensor,
        classes_pred: torch.Tensor,
        classes_true: torch.Tensor,
        psi_pred: Optional[torch.Tensor] = None,
        kappa_pred: Optional[torch.Tensor] = None,
        alpha_pred: Optional[torch.Tensor] = None,
        grid_coords: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        Compute total physics-constrained loss.
        
        Parameters
        ----------
        params_pred : torch.Tensor
            Predicted NFW parameters [B, N_params]
        params_true : torch.Tensor
            True parameters [B, N_params]
        classes_pred : torch.Tensor
            Predicted class logits [B, N_classes]
        classes_true : torch.Tensor
            True class labels [B] (integers)
        psi_pred : torch.Tensor, optional
            Predicted lensing potential [B, 1, H, W]
        kappa_pred : torch.Tensor, optional
            Predicted convergence [B, 1, H, W]
        alpha_pred : torch.Tensor, optional
            Predicted deflection [B, 2, H, W]
        grid_coords : torch.Tensor, optional
            Coordinate grid [B, 2, H, W] with requires_grad=True
            
        Returns
        -------
        total_loss : torch.Tensor
            Combined loss
        loss_dict : Dict[str, float]
            Individual loss components for monitoring
            
        Examples
        --------
        >>> loss_fn = PhysicsConstrainedPINNLoss()
        >>> total_loss, components = loss_fn(
        ...     params_pred=predicted_params,
        ...     params_true=true_params,
        ...     classes_pred=logits,
        ...     classes_true=labels,
        ...     psi_pred=potential_map,
        ...     kappa_pred=convergence_map,
        ...     alpha_pred=deflection_field,
        ...     grid_coords=coords  # with requires_grad=True
        ... )
        >>> print(f"Poisson loss: {components['poisson']:.4f}")
        >>> print(f"Gradient loss: {components['gradient']:.4f}")
        """
        # 1. Data loss: Parameter MSE
        param_loss = F.mse_loss(params_pred, params_true)
        
        # 2. Classification loss
        class_loss = F.cross_entropy(classes_pred, classes_true)
        
        # 3. Physics constraints (if provided)
        if psi_pred is not None and kappa_pred is not None:
            # Poisson equation: ∇²ψ = 2κ
            poisson_loss = self.poisson_loss(psi_pred, kappa_pred, grid_coords)
        else:
            poisson_loss = torch.tensor(0.0, device=params_pred.device)
        
        if psi_pred is not None and alpha_pred is not None:
            # Gradient consistency: α = ∇ψ
            gradient_loss = self.gradient_consistency_loss(psi_pred, alpha_pred, grid_coords)
        else:
            gradient_loss = torch.tensor(0.0, device=params_pred.device)
        
        if kappa_pred is not None:
            # Mass conservation
            conservation_loss = self.mass_conservation_loss(kappa_pred)
        else:
            conservation_loss = torch.tensor(0.0, device=params_pred.device)
        
        # 4. Regularization
        reg_loss = self.parameter_regularization(params_pred)
        
        # 5. Total loss
        total_loss = (
            param_loss +
            self.lambda_classification * class_loss +
            self.lambda_poisson * poisson_loss +
            self.lambda_gradient * gradient_loss +
            self.lambda_conservation * conservation_loss +
            self.lambda_reg * reg_loss
        )
        
        # 6. Loss dictionary for monitoring
        loss_dict = {
            'total': total_loss.item(),
            'parameter_mse': param_loss.item(),
            'classification': class_loss.item(),
            'poisson': poisson_loss.item() if isinstance(poisson_loss, torch.Tensor) else 0.0,
            'gradient': gradient_loss.item() if isinstance(gradient_loss, torch.Tensor) else 0.0,
            'conservation': conservation_loss.item() if isinstance(conservation_loss, torch.Tensor) else 0.0,
            'regularization': reg_loss.item()
        }
        
        return total_loss, loss_dict


def create_coordinate_grid(
    height: int,
    width: int,
    batch_size: int = 1,
    device: str = 'cpu',
    requires_grad: bool = True
) -> torch.Tensor:
    """
    Create coordinate grid for autograd-based derivative computation.
    
    Parameters
    ----------
    height : int
        Grid height
    width : int
        Grid width
    batch_size : int
        Batch size
    device : str
        Device ('cpu' or 'cuda')
    requires_grad : bool
        Whether to enable gradients (MUST be True for autograd derivatives)
        
    Returns
    -------
    grid : torch.Tensor
        Coordinate grid [B, 2, H, W]
        grid[:, 0] = x coordinates
        grid[:, 1] = y coordinates
        
    Examples
    --------
    >>> coords = create_coordinate_grid(64, 64, batch_size=8, requires_grad=True)
    >>> # Now can compute derivatives: ∂psi/∂coords
    """
    # Create meshgrid
    y = torch.linspace(-1, 1, height, device=device)
    x = torch.linspace(-1, 1, width, device=device)
    yy, xx = torch.meshgrid(y, x, indexing='ij')
    
    # Stack to [2, H, W]
    grid = torch.stack([xx, yy], dim=0)
    
    # Expand to batch
    grid = grid.unsqueeze(0).repeat(batch_size, 1, 1, 1)  # [B, 2, H, W]
    
    # Enable gradients
    grid.requires_grad = requires_grad
    
    return grid


# =============================================================================
# Validation utilities
# =============================================================================

def validate_poisson_equation(
    psi: torch.Tensor,
    kappa: torch.Tensor,
    grid_coords: torch.Tensor,
    tolerance: float = 0.1
) -> Dict[str, float]:
    """
    Validate that ∇²ψ ≈ 2κ for a trained model.
    
    Parameters
    ----------
    psi : torch.Tensor
        Lensing potential
    kappa : torch.Tensor
        Convergence
    grid_coords : torch.Tensor
        Coordinates with requires_grad=True
    tolerance : float
        Acceptable relative error
        
    Returns
    -------
    validation : dict
        - 'max_error': Maximum absolute error
        - 'mean_error': Mean absolute error  
        - 'relative_error': Relative error (%)
        - 'passed': bool, whether validation passed
    """
    loss_fn = PhysicsConstrainedPINNLoss(use_autograd=True)
    
    # Compute Laplacian
    laplacian = loss_fn.compute_laplacian_autograd(psi, grid_coords)
    target = 2.0 * kappa
    
    # Errors
    abs_error = torch.abs(laplacian - target)
    max_error = abs_error.max().item()
    mean_error = abs_error.mean().item()
    
    # Relative error
    rel_error = (abs_error / (torch.abs(target) + 1e-8)).mean().item() * 100
    
    passed = rel_error < (tolerance * 100)
    
    validation = {
        'max_error': max_error,
        'mean_error': mean_error,
        'relative_error': rel_error,
        'passed': passed,
        'message': f"Poisson equation: {rel_error:.2f}% error ({'PASS' if passed else 'FAIL'})"
    }
    
    return validation


def validate_gradient_consistency(
    psi: torch.Tensor,
    alpha: torch.Tensor,
    grid_coords: torch.Tensor,
    tolerance: float = 0.1
) -> Dict[str, float]:
    """
    Validate that α ≈ ∇ψ.
    
    Parameters
    ----------
    psi : torch.Tensor
        Lensing potential [B, 1, H, W]
    alpha : torch.Tensor
        Deflection angle [B, 2, H, W]
    grid_coords : torch.Tensor
        Coordinates
    tolerance : float
        Acceptable relative error
        
    Returns
    -------
    validation : dict
        Validation results
    """
    loss_fn = PhysicsConstrainedPINNLoss(use_autograd=True)
    
    # Compute gradient
    dpsi_dx, dpsi_dy = loss_fn.compute_gradient_autograd(psi, grid_coords)
    grad_psi = torch.cat([dpsi_dx, dpsi_dy], dim=1)
    
    # Errors
    abs_error = torch.abs(alpha - grad_psi)
    max_error = abs_error.max().item()
    mean_error = abs_error.mean().item()
    
    rel_error = (abs_error / (torch.abs(grad_psi) + 1e-8)).mean().item() * 100
    
    passed = rel_error < (tolerance * 100)
    
    validation = {
        'max_error': max_error,
        'mean_error': mean_error,
        'relative_error': rel_error,
        'passed': passed,
        'message': f"Gradient consistency: {rel_error:.2f}% error ({'PASS' if passed else 'FAIL'})"
    }
    
    return validation
