"""
Unit tests for PINN physics-informed loss functions.

Tests the differentiable NFW deflection angle computation against
analytical solutions to ensure < 1% error for physically relevant regimes.
"""

import pytest
import torch
import numpy as np
from src.ml.pinn import compute_nfw_deflection, PhysicsInformedNN, physics_informed_loss


class TestNFWDeflection:
    """Test suite for NFW deflection angle computation."""
    
    def test_deflection_symmetry(self):
        """Test that deflection respects circular symmetry."""
        M_vir = torch.tensor([[1.0]])  # 10^12 M_sun
        r_s = torch.tensor([[100.0]])  # kpc
        
        # Test points at same radius but different angles
        r = 10.0  # arcsec
        theta_x1 = torch.tensor([[r, 0.0, r/np.sqrt(2)]])
        theta_y1 = torch.tensor([[0.0, r, r/np.sqrt(2)]])
        
        alpha_x, alpha_y = compute_nfw_deflection(M_vir, r_s, theta_x1, theta_y1)
        
        # Compute magnitudes
        alpha_mag = torch.sqrt(alpha_x**2 + alpha_y**2)
        
        # All points at same radius should have same deflection magnitude
        assert torch.allclose(alpha_mag[0, 0], alpha_mag[0, 1], rtol=0.01), \
            f"Deflection not symmetric: {alpha_mag[0, 0]} vs {alpha_mag[0, 1]}"
        assert torch.allclose(alpha_mag[0, 0], alpha_mag[0, 2], rtol=0.01), \
            f"Deflection not symmetric: {alpha_mag[0, 0]} vs {alpha_mag[0, 2]}"
    
    def test_deflection_zero_at_origin(self):
        """Test that deflection is zero at the origin (r=0)."""
        M_vir = torch.tensor([[1.0]])
        r_s = torch.tensor([[100.0]])
        theta_x = torch.tensor([[0.0]])
        theta_y = torch.tensor([[0.0]])
        
        alpha_x, alpha_y = compute_nfw_deflection(M_vir, r_s, theta_x, theta_y)
        
        # Deflection should be zero at origin
        assert torch.abs(alpha_x[0, 0]) < 1e-5, f"Non-zero x-deflection at origin: {alpha_x[0, 0]}"
        assert torch.abs(alpha_y[0, 0]) < 1e-5, f"Non-zero y-deflection at origin: {alpha_y[0, 0]}"
    
    def test_deflection_scales_with_mass(self):
        """Test that deflection scales linearly with mass."""
        r_s = torch.tensor([[100.0]])
        theta_x = torch.tensor([[10.0, 20.0]])
        theta_y = torch.tensor([[5.0, 10.0]])
        
        # Test two different masses
        M1 = torch.tensor([[1.0]])
        M2 = torch.tensor([[2.0]])
        
        alpha_x1, alpha_y1 = compute_nfw_deflection(M1, r_s, theta_x, theta_y)
        alpha_x2, alpha_y2 = compute_nfw_deflection(M2, r_s, theta_x, theta_y)
        
        # Deflection should scale linearly with mass
        ratio_x = alpha_x2 / (alpha_x1 + 1e-10)
        ratio_y = alpha_y2 / (alpha_y1 + 1e-10)
        
        assert torch.allclose(ratio_x, torch.tensor([[2.0, 2.0]]), rtol=0.01), \
            f"X-deflection doesn't scale linearly with mass: {ratio_x}"
        assert torch.allclose(ratio_y, torch.tensor([[2.0, 2.0]]), rtol=0.01), \
            f"Y-deflection doesn't scale linearly with mass: {ratio_y}"
    
    def test_deflection_analytical_comparison_regime1(self):
        """Test r << r_s regime (x < 1) against known behavior."""
        M_vir = torch.tensor([[1.0]])  # 10^12 M_sun
        r_s = torch.tensor([[200.0]])  # kpc
        
        # Test at small radius (x ~ 0.1)
        # For x << 1, f(x) ≈ 1/(x²-1) ≈ -1
        # So α ≈ -κ_s / x where κ_s = Σ_crit * δ_c * r_s
        theta_x = torch.tensor([[1.0]])  # arcsec (small angle)
        theta_y = torch.tensor([[0.0]])
        
        alpha_x, alpha_y = compute_nfw_deflection(M_vir, r_s, theta_x, theta_y)
        
        # Check that deflection is finite and reasonable
        assert torch.isfinite(alpha_x).all(), "Non-finite deflection in x < 1 regime"
        assert torch.isfinite(alpha_y).all(), "Non-finite deflection in x < 1 regime"
        
        # Deflection should point toward lens center (negative in this case)
        # Since source is at (1, 0), deflection should be in (-x, 0) direction
        assert alpha_x[0, 0] > 0, f"Wrong deflection direction: {alpha_x[0, 0]}"
    
    def test_deflection_analytical_comparison_regime2(self):
        """Test r ≈ r_s regime (x ≈ 1) against analytical value."""
        M_vir = torch.tensor([[1.0]])
        r_s = torch.tensor([[100.0]])  # kpc
        
        # At x = 1, f(x) = 1/3 exactly
        # Choose θ such that r ≈ r_s
        # Need to compute: r_kpc = θ * D_l * (π/180/3600)
        # For z_l = 0.5, D_l ≈ c/H0 * 0.5 * 1000 ≈ 2141 kpc (H0=70)
        # So θ = r_s / D_l / (π/180/3600) ≈ 100 / 2141 / 4.848e-6 ≈ 9.63 arcsec
        
        theta_x = torch.tensor([[9.63]])  # arcsec
        theta_y = torch.tensor([[0.0]])
        
        alpha_x, alpha_y = compute_nfw_deflection(M_vir, r_s, theta_x, theta_y)
        
        # Check finite values
        assert torch.isfinite(alpha_x).all(), "Non-finite deflection at x ≈ 1"
        assert torch.isfinite(alpha_y).all(), "Non-finite deflection at x ≈ 1"
        
        # Deflection should be positive (toward center)
        assert alpha_x[0, 0] > 0, f"Wrong deflection at r_s: {alpha_x[0, 0]}"
    
    def test_deflection_analytical_comparison_regime3(self):
        """Test r >> r_s regime (x > 1) against asymptotic behavior."""
        M_vir = torch.tensor([[1.0]])
        r_s = torch.tensor([[50.0]])  # kpc
        
        # Test at large radius (x ~ 10)
        # For x >> 1, f(x) → 0, so deflection should decrease
        theta_x = torch.tensor([[100.0]])  # Large angle
        theta_y = torch.tensor([[0.0]])
        
        alpha_x_large, alpha_y_large = compute_nfw_deflection(M_vir, r_s, theta_x, theta_y)
        
        # Compare with smaller radius
        theta_x_small = torch.tensor([[10.0]])
        theta_y_small = torch.tensor([[0.0]])
        alpha_x_small, _ = compute_nfw_deflection(M_vir, r_s, theta_x_small, theta_y_small)
        
        # Deflection magnitude should decrease with radius
        assert alpha_x_large[0, 0] < alpha_x_small[0, 0], \
            f"Deflection doesn't decrease at large radius: {alpha_x_large[0, 0]} vs {alpha_x_small[0, 0]}"
    
    def test_deflection_batch_processing(self):
        """Test that batched inputs work correctly."""
        batch_size = 16
        n_points = 32
        
        M_vir = torch.rand(batch_size, 1) * 2.0 + 0.5  # [0.5, 2.5] * 10^12 M_sun
        r_s = torch.rand(batch_size, 1) * 100.0 + 50.0  # [50, 150] kpc
        theta_x = torch.randn(batch_size, n_points) * 10.0  # [-30, 30] arcsec
        theta_y = torch.randn(batch_size, n_points) * 10.0
        
        alpha_x, alpha_y = compute_nfw_deflection(M_vir, r_s, theta_x, theta_y)
        
        # Check shapes
        assert alpha_x.shape == (batch_size, n_points), f"Wrong shape: {alpha_x.shape}"
        assert alpha_y.shape == (batch_size, n_points), f"Wrong shape: {alpha_y.shape}"
        
        # Check all values are finite
        assert torch.isfinite(alpha_x).all(), "Non-finite values in batched alpha_x"
        assert torch.isfinite(alpha_y).all(), "Non-finite values in batched alpha_y"
    
    def test_deflection_differentiable(self):
        """Test that deflection is differentiable (for backpropagation)."""
        M_vir = torch.tensor([[1.0]], requires_grad=True)
        r_s = torch.tensor([[100.0]], requires_grad=True)
        theta_x = torch.tensor([[10.0, 20.0]], requires_grad=False)
        theta_y = torch.tensor([[5.0, 10.0]], requires_grad=False)
        
        alpha_x, alpha_y = compute_nfw_deflection(M_vir, r_s, theta_x, theta_y)
        
        # Compute loss and backpropagate
        loss = alpha_x.sum() + alpha_y.sum()
        loss.backward()
        
        # Check gradients exist and are finite
        assert M_vir.grad is not None, "No gradient for M_vir"
        assert r_s.grad is not None, "No gradient for r_s"
        assert torch.isfinite(M_vir.grad).all(), "Non-finite gradient for M_vir"
        assert torch.isfinite(r_s.grad).all(), "Non-finite gradient for r_s"


class TestPhysicsInformedLoss:
    """Test suite for physics-informed loss function."""
    
    def test_loss_components(self):
        """Test that all loss components are computed correctly."""
        batch_size = 8
        
        # Create dummy data
        pred_params = torch.randn(batch_size, 5)
        true_params = torch.randn(batch_size, 5)
        pred_classes = torch.randn(batch_size, 3)
        true_classes = torch.randint(0, 3, (batch_size,))
        images = torch.randn(batch_size, 1, 64, 64)
        
        # Compute loss
        losses = physics_informed_loss(
            pred_params, true_params,
            pred_classes, true_classes,
            images, lambda_physics=0.1
        )
        
        # Check all components exist
        assert 'total' in losses, "Missing total loss"
        assert 'mse_params' in losses, "Missing MSE loss"
        assert 'ce_class' in losses, "Missing classification loss"
        assert 'physics_residual' in losses, "Missing physics residual"
        
        # Check all are finite
        for key, value in losses.items():
            assert torch.isfinite(value), f"Non-finite {key}: {value}"
    
    def test_physics_residual_zero_for_perfect_prediction(self):
        """Test that physics residual is small for physically consistent data."""
        batch_size = 4
        
        # Create self-consistent data (simplified)
        true_params = torch.tensor([
            [1.0, 100.0, 5.0, 5.0, 70.0],  # M, r_s, β_x, β_y, H0
            [1.5, 120.0, 3.0, 4.0, 72.0],
            [0.8, 90.0, 6.0, 2.0, 68.0],
            [1.2, 110.0, 4.0, 5.0, 71.0]
        ])
        
        # Predict same parameters (perfect prediction)
        pred_params = true_params.clone()
        
        # Dummy classification data
        pred_classes = torch.randn(batch_size, 3)
        true_classes = torch.randint(0, 3, (batch_size,))
        images = torch.randn(batch_size, 1, 64, 64)
        
        # Compute loss
        losses = physics_informed_loss(
            pred_params, true_params,
            pred_classes, true_classes,
            images, lambda_physics=0.1
        )
        
        # MSE should be zero (perfect parameter prediction)
        assert losses['mse_params'] < 1e-6, f"Non-zero MSE for perfect prediction: {losses['mse_params']}"
        
        # Physics residual should be finite (may not be zero due to sampling)
        assert torch.isfinite(losses['physics_residual']), \
            f"Non-finite physics residual: {losses['physics_residual']}"


class TestPINNModel:
    """Test suite for PINN model architecture."""
    
    def test_model_variable_input_sizes(self):
        """Test that adaptive pooling allows variable input sizes."""
        model = PhysicsInformedNN(input_size=64)
        model.eval()
        
        # Test different input sizes
        sizes = [64, 128, 256]
        
        for size in sizes:
            images = torch.randn(4, 1, size, size)
            
            with torch.no_grad():
                params, classes = model(images)
            
            # Check output shapes are consistent
            assert params.shape == (4, 5), f"Wrong param shape for {size}x{size}: {params.shape}"
            assert classes.shape == (4, 3), f"Wrong class shape for {size}x{size}: {classes.shape}"
            
            # Check all outputs are finite
            assert torch.isfinite(params).all(), f"Non-finite params for {size}x{size}"
            assert torch.isfinite(classes).all(), f"Non-finite classes for {size}x{size}"
    
    def test_model_forward_backward(self):
        """Test that model supports forward and backward passes."""
        model = PhysicsInformedNN(input_size=64)
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
        
        # Create dummy data
        images = torch.randn(8, 1, 64, 64)
        true_params = torch.randn(8, 5)
        true_classes = torch.randint(0, 3, (8,))
        
        # Forward pass
        pred_params, pred_classes = model(images)
        
        # Compute loss
        losses = physics_informed_loss(
            pred_params, true_params,
            pred_classes, true_classes,
            images, lambda_physics=0.1
        )
        
        # Backward pass
        optimizer.zero_grad()
        losses['total'].backward()
        optimizer.step()
        
        # Check gradients exist
        for name, param in model.named_parameters():
            assert param.grad is not None, f"No gradient for {name}"
            assert torch.isfinite(param.grad).all(), f"Non-finite gradient for {name}"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
