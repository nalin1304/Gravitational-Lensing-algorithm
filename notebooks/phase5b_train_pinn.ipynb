{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b72c459",
   "metadata": {},
   "source": [
    "# Phase 5b: Train Physics-Informed Neural Network\n",
    "\n",
    "This notebook trains the PINN model to:\n",
    "1. **Regress lens parameters**: M_vir, r_s, β_x, β_y, H₀\n",
    "2. **Classify dark matter type**: CDM, WDM, SIDM\n",
    "3. **Enforce physical constraints**: Lens equation via physics-informed loss\n",
    "\n",
    "## Training Configuration\n",
    "\n",
    "- **Model**: Dual-head CNN with encoder + dense layers\n",
    "- **Loss**: MSE (params) + CrossEntropy (class) + Physics residual\n",
    "- **Optimizer**: AdamW with learning rate scheduling\n",
    "- **Augmentation**: Random rotation, flip, brightness\n",
    "- **Batch size**: 32\n",
    "- **Epochs**: 50 (with early stopping)\n",
    "- **Device**: GPU if available, otherwise CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc28bd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Training Complete! ✓**\n",
    "\n",
    "The PINN model has been successfully trained with:\n",
    "- ✓ Physics-informed loss enforcing lens equation\n",
    "- ✓ Dual-head architecture for parameters + classification\n",
    "- ✓ Learning rate scheduling with ReduceLROnPlateau\n",
    "- ✓ Early stopping to prevent overfitting\n",
    "- ✓ Best model saved for evaluation\n",
    "\n",
    "**Next Steps:**\n",
    "1. Comprehensive evaluation on test set (`phase5c_evaluate.ipynb`)\n",
    "2. Analyze confusion matrix and calibration\n",
    "3. Compute parameter errors (MAE, RMSE, MAPE)\n",
    "4. Generate publication-quality plots\n",
    "\n",
    "**Model saved at:** `../models/best_pinn_model.pth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d32165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Get some validation samples\n",
    "val_images, val_params, val_labels = next(iter(val_loader))\n",
    "val_images = val_images.float().to(device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    pred_params, pred_class_logits = model(val_images)\n",
    "    pred_probs = torch.softmax(pred_class_logits, dim=1)\n",
    "    pred_classes = torch.argmax(pred_probs, dim=1)\n",
    "\n",
    "# Move to CPU for visualization\n",
    "val_images_cpu = val_images.cpu().numpy()\n",
    "val_params_cpu = val_params.numpy()\n",
    "val_labels_cpu = val_labels.numpy()\n",
    "pred_params_cpu = pred_params.cpu().numpy()\n",
    "pred_classes_cpu = pred_classes.cpu().numpy()\n",
    "pred_probs_cpu = pred_probs.cpu().numpy()\n",
    "\n",
    "# Visualize first 9 samples\n",
    "fig, axes = plt.subplots(3, 3, figsize=(14, 14))\n",
    "class_names = ['CDM', 'WDM', 'SIDM']\n",
    "param_names = ['M_vir', 'r_s', 'β_x', 'β_y', 'H₀']\n",
    "\n",
    "for i in range(9):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    \n",
    "    # Show image\n",
    "    ax.imshow(val_images_cpu[i, 0], cmap='viridis', origin='lower')\n",
    "    \n",
    "    # True vs predicted\n",
    "    true_class = class_names[val_labels_cpu[i]]\n",
    "    pred_class = class_names[pred_classes_cpu[i]]\n",
    "    confidence = pred_probs_cpu[i, pred_classes_cpu[i]] * 100\n",
    "    \n",
    "    # Check if classification is correct\n",
    "    correct = \"✓\" if val_labels_cpu[i] == pred_classes_cpu[i] else \"✗\"\n",
    "    color = 'green' if correct == \"✓\" else 'red'\n",
    "    \n",
    "    title = f\"{correct} True: {true_class} | Pred: {pred_class} ({confidence:.0f}%)\\n\"\n",
    "    title += f\"M: {val_params_cpu[i,0]:.2e} → {pred_params_cpu[i,0]:.2e}\\n\"\n",
    "    title += f\"H₀: {val_params_cpu[i,4]:.1f} → {pred_params_cpu[i,4]:.1f}\"\n",
    "    \n",
    "    ax.set_title(title, fontsize=9, color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Validation Samples: Predictions vs Ground Truth', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy on this batch\n",
    "accuracy = np.mean(val_labels_cpu == pred_classes_cpu) * 100\n",
    "print(f\"\\nBatch classification accuracy: {accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e48a2",
   "metadata": {},
   "source": [
    "## 7. Quick Validation on Test Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfba767",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Total loss\n",
    "ax = axes[0, 0]\n",
    "ax.plot(epochs_range, history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "ax.plot(epochs_range, history['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Total Loss', fontsize=12)\n",
    "ax.set_title('Total Loss', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter MSE\n",
    "ax = axes[0, 1]\n",
    "ax.plot(epochs_range, history['train_mse'], 'b-', label='Train', linewidth=2)\n",
    "ax.plot(epochs_range, history['val_mse'], 'r-', label='Validation', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('MSE Loss', fontsize=12)\n",
    "ax.set_title('Parameter Regression (MSE)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Classification CE\n",
    "ax = axes[1, 0]\n",
    "ax.plot(epochs_range, history['train_ce'], 'b-', label='Train', linewidth=2)\n",
    "ax.plot(epochs_range, history['val_ce'], 'r-', label='Validation', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "ax.set_title('Classification Loss', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Physics residual\n",
    "ax = axes[1, 1]\n",
    "ax.plot(epochs_range, history['train_physics'], 'b-', label='Train', linewidth=2)\n",
    "ax.plot(epochs_range, history['val_physics'], 'r-', label='Validation', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Physics Residual', fontsize=12)\n",
    "ax.set_title('Physics Constraint Violation', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training History - Physics-Informed Neural Network', fontsize=15, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Learning rate schedule\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(epochs_range, history['lr'], 'g-', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Learning Rate', fontsize=12)\n",
    "ax.set_title('Learning Rate Schedule', fontsize=13, fontweight='bold')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b93c16f",
   "metadata": {},
   "source": [
    "## 6. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c06f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'train_mse': [], 'train_ce': [], 'train_physics': [],\n",
    "    'val_loss': [], 'val_mse': [], 'val_ce': [], 'val_physics': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_path = '../models/best_pinn_model.pth'\n",
    "Path(best_model_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*25 + \"TRAINING START\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_losses = {'total': [], 'mse_params': [], 'ce_class': [], 'physics_residual': []}\n",
    "    \n",
    "    for batch_idx, (images, params, labels) in enumerate(train_loader):\n",
    "        images = images.float().to(device)\n",
    "        params = params.float().to(device)\n",
    "        labels = labels.long().to(device)\n",
    "        \n",
    "        # Training step\n",
    "        losses = train_step(model, images, params, labels, optimizer, LAMBDA_PHYSICS, device)\n",
    "        \n",
    "        for key in losses:\n",
    "            train_losses[key].append(losses[key])\n",
    "    \n",
    "    # Average training losses\n",
    "    avg_train_loss = np.mean(train_losses['total'])\n",
    "    avg_train_mse = np.mean(train_losses['mse_params'])\n",
    "    avg_train_ce = np.mean(train_losses['ce_class'])\n",
    "    avg_train_physics = np.mean(train_losses['physics_residual'])\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_losses = {'total': [], 'mse_params': [], 'ce_class': [], 'physics_residual': []}\n",
    "    \n",
    "    for images, params, labels in val_loader:\n",
    "        images = images.float().to(device)\n",
    "        params = params.float().to(device)\n",
    "        labels = labels.long().to(device)\n",
    "        \n",
    "        losses = validate_step(model, images, params, labels, LAMBDA_PHYSICS, device)\n",
    "        \n",
    "        for key in losses:\n",
    "            val_losses[key].append(losses[key])\n",
    "    \n",
    "    # Average validation losses\n",
    "    avg_val_loss = np.mean(val_losses['total'])\n",
    "    avg_val_mse = np.mean(val_losses['mse_params'])\n",
    "    avg_val_ce = np.mean(val_losses['ce_class'])\n",
    "    avg_val_physics = np.mean(val_losses['physics_residual'])\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(avg_val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['train_mse'].append(avg_train_mse)\n",
    "    history['train_ce'].append(avg_train_ce)\n",
    "    history['train_physics'].append(avg_train_physics)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['val_mse'].append(avg_val_mse)\n",
    "    history['val_ce'].append(avg_val_ce)\n",
    "    history['val_physics'].append(avg_val_physics)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] ({epoch_time:.1f}s)\")\n",
    "    print(f\"  Train - Loss: {avg_train_loss:.4f} | MSE: {avg_train_mse:.4f} | CE: {avg_train_ce:.4f} | Phys: {avg_train_physics:.4f}\")\n",
    "    print(f\"  Val   - Loss: {avg_val_loss:.4f} | MSE: {avg_val_mse:.4f} | CE: {avg_val_ce:.4f} | Phys: {avg_val_physics:.4f}\")\n",
    "    print(f\"  LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': avg_val_loss,\n",
    "        }, best_model_path)\n",
    "        print(f\"  ✓ Best model saved (val_loss: {best_val_loss:.4f})\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\n⚠ Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    print()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"=\"*70)\n",
    "print(f\"Training completed in {total_time/60:.1f} minutes\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbac93c",
   "metadata": {},
   "source": [
    "## 5. Training Loop with Live Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a92ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "LAMBDA_PHYSICS = 0.1\n",
    "PATIENCE = 10  # For early stopping\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler (reduce on plateau)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Physics weight (λ): {LAMBDA_PHYSICS}\")\n",
    "print(f\"  Optimizer: AdamW\")\n",
    "print(f\"  Scheduler: ReduceLROnPlateau\")\n",
    "print(f\"  Early stopping patience: {PATIENCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a651d20c",
   "metadata": {},
   "source": [
    "## 4. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aeee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = PhysicsInformedNN(input_size=64, dropout_rate=0.2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" \"*20 + \"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel structure:\")\n",
    "print(model)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6729c30",
   "metadata": {},
   "source": [
    "## 3. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = '../data/processed/lens_training_data.h5'\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = LensDataset(DATA_FILE, split='train')\n",
    "val_dataset = LensDataset(DATA_FILE, split='val')\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Training set: {len(train_dataset)} samples\")\n",
    "print(f\"Validation set: {len(val_dataset)} samples\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "\n",
    "# Test loading a batch\n",
    "images, params, labels = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  Images: {images.shape}\")\n",
    "print(f\"  Params: {params.shape}\")\n",
    "print(f\"  Labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3a8f3",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.ml import PhysicsInformedNN\n",
    "from src.ml.pinn import train_step, validate_step\n",
    "from src.ml.generate_dataset import LensDataset\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "print(\"✓ All modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ad416",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
