{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766217c0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Dataset Generation Complete! ✓**\n",
    "\n",
    "The synthetic training dataset has been successfully generated with:\n",
    "- ✓ Balanced class distribution (CDM, WDM, SIDM)\n",
    "- ✓ Realistic noise modeling (Gaussian + Poisson)\n",
    "- ✓ Diverse parameter ranges\n",
    "- ✓ Efficient HDF5 storage format\n",
    "- ✓ Train/validation/test splits\n",
    "\n",
    "**Next Steps:**\n",
    "1. Train the PINN model (`phase5b_train_pinn.ipynb`)\n",
    "2. Evaluate performance (`phase5c_evaluate.ipynb`)\n",
    "\n",
    "**To scale up:**\n",
    "- Increase `N_SAMPLES` to 100,000+ for production training\n",
    "- Add data augmentation (rotation, flipping, brightness)\n",
    "- Generate multiple datasets with different noise levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c91b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(OUTPUT_FILE).exists():\n",
    "    with h5py.File(OUTPUT_FILE, 'r') as f:\n",
    "        # Load training set for analysis\n",
    "        train_labels = f['train/labels'][:]\n",
    "        train_params = f['train/parameters'][:]\n",
    "        \n",
    "    print(\"=\"*70)\n",
    "    print(\" \"*20 + \"DATASET STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Class distribution\n",
    "    print(\"\\nCLASS DISTRIBUTION (Training Set):\")\n",
    "    class_names = ['CDM', 'WDM', 'SIDM']\n",
    "    for i, name in enumerate(class_names):\n",
    "        count = np.sum(train_labels == i)\n",
    "        percentage = count / len(train_labels) * 100\n",
    "        print(f\"  {name}: {count:,} samples ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Parameter statistics\n",
    "    print(\"\\nPARAMETER STATISTICS (Training Set):\")\n",
    "    param_names = ['M_vir', 'r_s', 'beta_x', 'beta_y', 'H0']\n",
    "    print(f\"\\n{'Parameter':<12} {'Mean':<15} {'Std':<15} {'Min':<15} {'Max':<15}\")\n",
    "    print(\"-\" * 72)\n",
    "    \n",
    "    for i, name in enumerate(param_names):\n",
    "        data = train_params[:, i]\n",
    "        print(f\"{name:<12} {data.mean():<15.3e} {data.std():<15.3e} {data.min():<15.3e} {data.max():<15.3e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    # Plot class distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart\n",
    "    ax = axes[0]\n",
    "    counts = [np.sum(train_labels == i) for i in range(3)]\n",
    "    bars = ax.bar(class_names, counts, color=['blue', 'orange', 'green'], alpha=0.7, edgecolor='black')\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title('Class Distribution', fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{count:,}',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Pie chart\n",
    "    ax = axes[1]\n",
    "    colors = ['blue', 'orange', 'green']\n",
    "    explode = (0.05, 0.05, 0.05)\n",
    "    ax.pie(counts, labels=class_names, autopct='%1.1f%%', colors=colors,\n",
    "           explode=explode, shadow=True, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "    ax.set_title('Class Proportion', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"❌ Dataset file not found. Please generate the dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831fad28",
   "metadata": {},
   "source": [
    "## 6. Dataset Statistics\n",
    "\n",
    "Analyze the class distribution and parameter statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9495f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file exists\n",
    "if Path(OUTPUT_FILE).exists():\n",
    "    print(f\"✓ HDF5 file found: {OUTPUT_FILE}\")\n",
    "    print(f\"  File size: {Path(OUTPUT_FILE).stat().st_size / 1024**2:.2f} MB\\n\")\n",
    "    \n",
    "    # Open and inspect\n",
    "    with h5py.File(OUTPUT_FILE, 'r') as f:\n",
    "        print(\"=\"*70)\n",
    "        print(\" \"*20 + \"DATASET STRUCTURE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nSplits available:\", list(f.keys()))\n",
    "        \n",
    "        for split in ['train', 'val', 'test']:\n",
    "            print(f\"\\n{split.upper()} SET:\")\n",
    "            print(f\"  Images shape:     {f[f'{split}/images'].shape}\")\n",
    "            print(f\"  Parameters shape: {f[f'{split}/parameters'].shape}\")\n",
    "            print(f\"  Labels shape:     {f[f'{split}/labels'].shape}\")\n",
    "        \n",
    "        print(\"\\n\\nMETADATA:\")\n",
    "        for key, value in f.attrs.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "        # Load a few samples for visualization\n",
    "        train_images = f['train/images'][:9]\n",
    "        train_params = f['train/parameters'][:9]\n",
    "        train_labels = f['train/labels'][:9]\n",
    "    \n",
    "    # Visualize samples from dataset\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    class_names = ['CDM', 'WDM', 'SIDM']\n",
    "    \n",
    "    for i in range(9):\n",
    "        ax = axes[i // 3, i % 3]\n",
    "        im = ax.imshow(train_images[i], cmap='viridis', origin='lower')\n",
    "        \n",
    "        label = train_labels[i]\n",
    "        params = train_params[i]\n",
    "        \n",
    "        title = f\"{class_names[label]}\\n\"\n",
    "        title += f\"M={params[0]:.2e}, H₀={params[4]:.1f}\"\n",
    "        \n",
    "        ax.set_title(title, fontsize=9)\n",
    "        ax.axis('off')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.suptitle('Random Samples from Training Set', fontsize=14, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(f\"❌ File not found: {OUTPUT_FILE}\")\n",
    "    print(\"   Please run the generation step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37230a75",
   "metadata": {},
   "source": [
    "## 5. Verify Generated Dataset\n",
    "\n",
    "Let's load and verify the generated HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3293e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "N_SAMPLES = 10000  # Start with 10K, increase to 100K for production\n",
    "OUTPUT_FILE = '../data/processed/lens_training_data.h5'\n",
    "GRID_SIZE = 64\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" \"*15 + \"DATASET GENERATION CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total samples:    {N_SAMPLES:,}\")\n",
    "print(f\"Grid size:        {GRID_SIZE}×{GRID_SIZE}\")\n",
    "print(f\"Output file:      {OUTPUT_FILE}\")\n",
    "print(f\"Train/Val/Test:   70%/15%/15%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Confirm before generating\n",
    "response = input(\"\\nProceed with generation? (yes/no): \")\n",
    "\n",
    "if response.lower() == 'yes':\n",
    "    print(\"\\n🚀 Starting dataset generation...\\n\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate dataset\n",
    "    split_info = generate_training_data(\n",
    "        n_samples=N_SAMPLES,\n",
    "        output_file=OUTPUT_FILE,\n",
    "        grid_size=GRID_SIZE,\n",
    "        train_split=0.7,\n",
    "        val_split=0.15,\n",
    "        test_split=0.15,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n✓ Dataset generation complete!\")\n",
    "    print(f\"  Total time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "    print(f\"  Samples per second: {N_SAMPLES/elapsed_time:.1f}\")\n",
    "    print(f\"\\n  Train samples: {split_info['train']:,}\")\n",
    "    print(f\"  Val samples:   {split_info['val']:,}\")\n",
    "    print(f\"  Test samples:  {split_info['test']:,}\")\n",
    "else:\n",
    "    print(\"\\n❌ Generation cancelled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0710c7",
   "metadata": {},
   "source": [
    "## 4. Generate Full Dataset\n",
    "\n",
    "Now let's generate the complete training dataset. \n",
    "\n",
    "**Configuration:**\n",
    "- Start with 10,000 samples (increase to 100,000+ for production)\n",
    "- 70% training, 15% validation, 15% test split\n",
    "- Saved to HDF5 format\n",
    "\n",
    "**Estimated time:**\n",
    "- 10K samples: ~5-10 minutes\n",
    "- 100K samples: ~1-2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 300 samples (100 per DM type)\n",
    "n_test_samples = 300\n",
    "test_params = []\n",
    "test_labels = []\n",
    "\n",
    "print(f\"Generating {n_test_samples} samples for parameter analysis...\")\n",
    "for i in range(n_test_samples):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"  {i}/{n_test_samples}...\")\n",
    "    \n",
    "    dm_type = ['CDM', 'WDM', 'SIDM'][i % 3]\n",
    "    try:\n",
    "        _, params, label = generate_single_sample(dm_type, grid_size=64, add_noise_flag=False)\n",
    "        test_params.append(params)\n",
    "        test_labels.append(label)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "test_params = np.array(test_params)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(f\"✓ Generated {len(test_params)} samples\")\n",
    "\n",
    "# Plot parameter distributions\n",
    "param_names = ['M_vir (M☉)', 'r_s (kpc)', 'β_x (arcsec)', 'β_y (arcsec)', 'H₀ (km/s/Mpc)']\n",
    "colors = ['blue', 'orange', 'green']\n",
    "dm_names = ['CDM', 'WDM', 'SIDM']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(5):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    for label_idx, (color, name) in enumerate(zip(colors, dm_names)):\n",
    "        mask = test_labels == label_idx\n",
    "        if np.sum(mask) > 0:\n",
    "            data = test_params[mask, i]\n",
    "            ax.hist(data, bins=20, alpha=0.6, color=color, label=name, edgecolor='black')\n",
    "    \n",
    "    ax.set_xlabel(param_names[i], fontsize=11)\n",
    "    ax.set_ylabel('Count', fontsize=11)\n",
    "    ax.set_title(f'{param_names[i]}\\nRange: [{test_params[:, i].min():.2e}, {test_params[:, i].max():.2e}]', fontsize=10)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "axes[5].remove()\n",
    "\n",
    "plt.suptitle('Parameter Distributions by Dark Matter Type', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb493a8",
   "metadata": {},
   "source": [
    "## 3. Visualize Parameter Distributions\n",
    "\n",
    "Let's generate a small batch to visualize the parameter distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ab15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample images for each DM type\n",
    "dm_types = ['CDM', 'WDM', 'SIDM']\n",
    "n_samples_per_type = 3\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "print(\"Generating sample images...\")\n",
    "for i, dm_type in enumerate(dm_types):\n",
    "    for j in range(n_samples_per_type):\n",
    "        print(f\"  {dm_type} sample {j+1}/3...\", end=\" \")\n",
    "        try:\n",
    "            image, params, label = generate_single_sample(dm_type, grid_size=64, add_noise_flag=True)\n",
    "            \n",
    "            ax = axes[i, j]\n",
    "            im = ax.imshow(image, cmap='viridis', origin='lower')\n",
    "            ax.set_title(f'{dm_type} (label={label})\\nM={params[0]:.2e} M☉', fontsize=10)\n",
    "            ax.axis('off')\n",
    "            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            print(\"✓\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {e}\")\n",
    "\n",
    "plt.suptitle('Sample Convergence Maps by Dark Matter Type', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Sample visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d984c9",
   "metadata": {},
   "source": [
    "## 2. Visualize Sample Images\n",
    "\n",
    "Before generating the full dataset, let's visualize a few samples from each dark matter type to verify the data generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b13ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.ml import generate_training_data\n",
    "from src.ml.generate_dataset import generate_single_sample\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"✓ All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e812cb8",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771eeec",
   "metadata": {},
   "source": [
    "# Phase 5a: Generate Training Dataset for PINN\n",
    "\n",
    "This notebook generates a large-scale synthetic dataset for training the Physics-Informed Neural Network (PINN) to:\n",
    "1. Infer lens parameters (M_vir, r_s, source position, H₀)\n",
    "2. Classify dark matter model type (CDM, WDM, SIDM)\n",
    "\n",
    "## Dataset Specification\n",
    "\n",
    "- **Size**: 10,000 samples (configurable up to 100,000+)\n",
    "- **Image resolution**: 64×64 pixels\n",
    "- **Dark matter types**: 33% CDM, 33% WDM, 34% SIDM\n",
    "- **Noise**: Realistic Gaussian + Poisson noise\n",
    "- **Split**: 70% train, 15% validation, 15% test\n",
    "- **Format**: HDF5 for efficient storage and loading"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
