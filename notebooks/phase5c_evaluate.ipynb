{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37f7cef",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Evaluation Complete! âœ“**\n",
    "\n",
    "The PINN model has been comprehensively evaluated on the test set:\n",
    "\n",
    "**Key Results:**\n",
    "- âœ“ Classification accuracy computed\n",
    "- âœ“ Confusion matrix visualized  \n",
    "- âœ“ Parameter errors analyzed (MAE, RMSE, MAPE)\n",
    "- âœ“ Calibration curves generated\n",
    "- âœ“ Scatter plots created\n",
    "- âœ“ Performance compared against targets\n",
    "\n",
    "**Files Generated:**\n",
    "- Comprehensive evaluation metrics\n",
    "- Multiple publication-quality visualizations\n",
    "- Error analysis by dark matter type\n",
    "- Best/worst prediction examples\n",
    "\n",
    "**Next Steps:**\n",
    "1. If targets met: Deploy model for inference\n",
    "2. If not: Apply enhancements (augmentation, tuning, more data)\n",
    "3. Add ML unit tests for robustness\n",
    "4. Integrate TensorBoard for training monitoring\n",
    "5. Document results for publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare against target metrics\n",
    "metrics = results['metrics']\n",
    "classification_acc = metrics['classification']['overall_accuracy']\n",
    "avg_mape = metrics['parameters']['overall_mape']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*15 + \"PERFORMANCE vs TARGET METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Metric':<40s} {'Target':<15s} {'Achieved':<15s} {'Status':<10s}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Classification accuracy\n",
    "target_acc = 90.0\n",
    "status_acc = \"âœ“ PASS\" if classification_acc >= target_acc else \"âœ— FAIL\"\n",
    "print(f\"{'Classification Accuracy (%)':<40s} {f'>= {target_acc:.1f}':<15s} {f'{classification_acc:.2f}':<15s} {status_acc:<10s}\")\n",
    "\n",
    "# Parameter error\n",
    "target_error = 5.0\n",
    "status_error = \"âœ“ PASS\" if avg_mape <= target_error else \"âœ— FAIL\"\n",
    "print(f\"{'Parameter Error (MAPE %)':<40s} {f'< {target_error:.1f}':<15s} {f'{avg_mape:.2f}':<15s} {status_error:<10s}\")\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for dm_type, class_name in enumerate(class_names):\n",
    "    class_acc = metrics['classification']['per_class_accuracy'][dm_type]\n",
    "    status = \"âœ“\" if class_acc >= target_acc else \"âœ—\"\n",
    "    print(f\"  {status} {class_name}: {class_acc:.2f}%\")\n",
    "\n",
    "# Per-parameter MAPE\n",
    "print(\"\\nPer-Parameter MAPE:\")\n",
    "for i, param_name in enumerate(param_names):\n",
    "    param_mape = metrics['parameters']['per_parameter_mape'][i]\n",
    "    status = \"âœ“\" if param_mape <= target_error else \"âœ—\"\n",
    "    print(f\"  {status} {param_name}: {param_mape:.2f}%\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Final summary\n",
    "if classification_acc >= target_acc and avg_mape <= target_error:\n",
    "    print(\"\\nðŸŽ‰ SUCCESS! All target metrics achieved!\")\n",
    "    print(\"   The PINN model is ready for deployment.\")\n",
    "else:\n",
    "    print(\"\\nâš  Some targets not met. Consider:\")\n",
    "    if classification_acc < target_acc:\n",
    "        print(\"   - Increase training data\")\n",
    "        print(\"   - Adjust classification loss weight\")\n",
    "    if avg_mape > target_error:\n",
    "        print(\"   - Increase physics loss weight (Î»)\")\n",
    "        print(\"   - Add more augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7af243",
   "metadata": {},
   "source": [
    "## 10. Performance vs Target Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate combined error metric (normalized MAE for params + classification error)\n",
    "param_errors = np.mean(np.abs(pred_params - true_params), axis=1)\n",
    "class_errors = (results['predictions']['pred_classes'] != true_classes).astype(float)\n",
    "combined_error = param_errors + class_errors * np.mean(param_errors) * 2  # Weight class errors higher\n",
    "\n",
    "# Find best and worst predictions\n",
    "best_indices = np.argsort(combined_error)[:6]\n",
    "worst_indices = np.argsort(combined_error)[-6:]\n",
    "\n",
    "# Load original images for visualization\n",
    "import h5py\n",
    "with h5py.File(DATA_FILE, 'r') as f:\n",
    "    test_images = f['test/images'][:]\n",
    "\n",
    "# Plot best predictions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, idx in enumerate(best_indices):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(test_images[idx], cmap='viridis', origin='lower')\n",
    "    \n",
    "    true_class = class_names[true_classes[idx]]\n",
    "    pred_class = class_names[results['predictions']['pred_classes'][idx]]\n",
    "    confidence = results['predictions']['class_probs'][idx, results['predictions']['pred_classes'][idx]] * 100\n",
    "    \n",
    "    mae = param_errors[idx]\n",
    "    \n",
    "    title = f\"âœ“ {pred_class} ({confidence:.0f}%) | MAE: {mae:.3f}\\n\"\n",
    "    title += f\"True: {true_class} | M_vir: {true_params[idx,0]:.2e}\"\n",
    "    \n",
    "    ax.set_title(title, fontsize=9, color='green', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Best Predictions (Lowest Error)', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot worst predictions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, idx in enumerate(worst_indices):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(test_images[idx], cmap='viridis', origin='lower')\n",
    "    \n",
    "    true_class = class_names[true_classes[idx]]\n",
    "    pred_class = class_names[results['predictions']['pred_classes'][idx]]\n",
    "    confidence = results['predictions']['class_probs'][idx, results['predictions']['pred_classes'][idx]] * 100\n",
    "    \n",
    "    mae = param_errors[idx]\n",
    "    correct = \"âœ“\" if true_classes[idx] == results['predictions']['pred_classes'][idx] else \"âœ—\"\n",
    "    color = 'orange' if correct == \"âœ“\" else 'red'\n",
    "    \n",
    "    title = f\"{correct} {pred_class} ({confidence:.0f}%) | MAE: {mae:.3f}\\n\"\n",
    "    title += f\"True: {true_class} | M_vir: {true_params[idx,0]:.2e}\"\n",
    "    \n",
    "    ax.set_title(title, fontsize=9, color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Worst Predictions (Highest Error)', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40114d21",
   "metadata": {},
   "source": [
    "## 9. Best and Worst Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dace762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors by DM type\n",
    "pred_params = results['predictions']['pred_params']\n",
    "true_params = results['predictions']['true_params']\n",
    "true_classes = results['predictions']['true_classes']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for dm_type in range(3):\n",
    "    mask = true_classes == dm_type\n",
    "    \n",
    "    # Calculate MAE for each parameter\n",
    "    errors = np.abs(pred_params[mask] - true_params[mask])\n",
    "    mae_per_param = np.mean(errors, axis=0)\n",
    "    \n",
    "    ax = axes[dm_type]\n",
    "    x = np.arange(5)\n",
    "    ax.bar(x, mae_per_param, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['M_vir', 'r_s', 'Î²_x', 'Î²_y', 'Hâ‚€'], rotation=45)\n",
    "    ax.set_ylabel('Mean Absolute Error', fontsize=11)\n",
    "    ax.set_title(f'{class_names[dm_type]} - Parameter Errors', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Parameter Prediction Errors by Dark Matter Type', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nError Statistics by Dark Matter Type:\")\n",
    "print(\"=\"*70)\n",
    "for dm_type in range(3):\n",
    "    mask = true_classes == dm_type\n",
    "    errors = np.abs(pred_params[mask] - true_params[mask])\n",
    "    mae_overall = np.mean(errors)\n",
    "    print(f\"{class_names[dm_type]:5s} - Overall MAE: {mae_overall:.4f} | Samples: {mask.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4257e",
   "metadata": {},
   "source": [
    "## 8. Error Analysis by Dark Matter Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b123c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_parameter_scatter(\n",
    "    results['predictions']['pred_params'],\n",
    "    results['predictions']['true_params'],\n",
    "    param_names\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadecf6d",
   "metadata": {},
   "source": [
    "## 7. Parameter Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52225dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_calibration_curve(\n",
    "    results['predictions']['class_probs'],\n",
    "    results['predictions']['true_classes'],\n",
    "    class_names\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9846b70d",
   "metadata": {},
   "source": [
    "## 6. Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = ['M_vir [Mâ˜‰]', 'r_s [kpc]', 'Î²_x [arcsec]', 'Î²_y [arcsec]', 'Hâ‚€ [km/s/Mpc]']\n",
    "\n",
    "fig = plot_parameter_errors(\n",
    "    results['predictions']['pred_params'],\n",
    "    results['predictions']['true_params'],\n",
    "    param_names,\n",
    "    results['metrics']['parameters']\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842aec8b",
   "metadata": {},
   "source": [
    "## 5. Parameter Prediction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb95390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['CDM', 'WDM', 'SIDM']\n",
    "\n",
    "# Plot confusion matrix (counts)\n",
    "fig = plot_confusion_matrix(\n",
    "    results['metrics']['classification']['confusion_matrix'],\n",
    "    class_names,\n",
    "    normalize=False\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "fig = plot_confusion_matrix(\n",
    "    results['metrics']['classification']['confusion_matrix'],\n",
    "    class_names,\n",
    "    normalize=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a081466",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b676dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "results = evaluate_model(model, test_loader, device, return_predictions=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*20 + \"EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print_evaluation_summary(results['metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38730a",
   "metadata": {},
   "source": [
    "## 3. Run Complete Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb2f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "MODEL_PATH = '../models/best_pinn_model.pth'\n",
    "model = PhysicsInformedNN(input_size=64, dropout_rate=0.2)\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ“ Model loaded from: {MODEL_PATH}\")\n",
    "print(f\"  Trained for {checkpoint['epoch']+1} epochs\")\n",
    "print(f\"  Best validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "# Load test dataset\n",
    "DATA_FILE = '../data/processed/lens_training_data.h5'\n",
    "test_dataset = LensDataset(DATA_FILE, split='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nâœ“ Test dataset loaded: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de28a0",
   "metadata": {},
   "source": [
    "## 2. Load Model and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b23319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.ml import PhysicsInformedNN, evaluate_model, compute_metrics\n",
    "from src.ml.evaluate import (\n",
    "    plot_confusion_matrix, plot_parameter_errors,\n",
    "    plot_calibration_curve, plot_parameter_scatter,\n",
    "    print_evaluation_summary\n",
    ")\n",
    "from src.ml.generate_dataset import LensDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "print(\"âœ“ All modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12a0ef",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca287f3",
   "metadata": {},
   "source": [
    "# Phase 5c: Comprehensive Model Evaluation\n",
    "\n",
    "This notebook performs comprehensive evaluation of the trained PINN model on the test set:\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- Classification: Overall accuracy, per-class accuracy, confusion matrix\n",
    "- Regression: MAE, RMSE, MAPE for each parameter\n",
    "- Calibration: Probability calibration curves\n",
    "- Visualization: Prediction scatter plots, error distributions\n",
    "\n",
    "**Expected Performance:**\n",
    "- Classification accuracy: >90%\n",
    "- Parameter error: <5%\n",
    "\n",
    "Let's see how our physics-informed model performs!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
