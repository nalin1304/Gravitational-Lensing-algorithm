{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde93494",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Advanced Training Complete! ✓**\n",
    "\n",
    "**New Features Demonstrated:**\n",
    "1. ✓ **Data Augmentation**: Rotation, flip, brightness, noise\n",
    "2. ✓ **TensorBoard Logging**: All metrics logged in real-time\n",
    "3. ✓ **Learning Rate Scheduling**: Adaptive LR with ReduceLROnPlateau\n",
    "4. ✓ **Early Stopping**: Training stopped when validation plateaus\n",
    "5. ✓ **Model Checkpointing**: Best model saved automatically\n",
    "6. ✓ **Gradient Clipping**: Built into train_step for stability\n",
    "\n",
    "**Benefits Over Basic Training:**\n",
    "- Better generalization through data augmentation\n",
    "- Real-time monitoring via TensorBoard\n",
    "- More stable training with LR scheduling\n",
    "- Automatic early stopping prevents overfitting\n",
    "- Production-ready training pipeline\n",
    "\n",
    "**View Results:**\n",
    "Run in terminal: `tensorboard --logdir=../runs`\n",
    "Then open: http://localhost:6006\n",
    "\n",
    "**Next Steps:**\n",
    "1. Compare augmented vs non-augmented performance\n",
    "2. Tune augmentation parameters\n",
    "3. Try different LR schedules (CosineAnnealing, etc.)\n",
    "4. Experiment with λ_physics weight\n",
    "5. Evaluate on test set (phase5c_evaluate.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf4e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 50\n",
    "LAMBDA_PHYSICS = 0.1\n",
    "PATIENCE = 10\n",
    "LOG_INTERVAL = 5  # Log predictions every N epochs\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_path = '../models/best_pinn_augmented.pth'\n",
    "Path(best_model_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*20 + \"ADVANCED TRAINING START\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Epochs: {NUM_EPOCHS} | Patience: {PATIENCE} | λ_physics: {LAMBDA_PHYSICS}\")\n",
    "print(\"Features: Augmentation ✓ | TensorBoard ✓ | LR Scheduling ✓\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_losses = {'total': [], 'mse_params': [], 'ce_class': [], 'physics_residual': []}\n",
    "    \n",
    "    for batch_idx, (images, params, labels) in enumerate(train_loader):\n",
    "        # Convert augmented images to tensors if needed\n",
    "        if isinstance(images, np.ndarray):\n",
    "            images = torch.from_numpy(images).float()\n",
    "        images = images.to(device)\n",
    "        params = params.float().to(device)\n",
    "        labels = labels.long().to(device)\n",
    "        \n",
    "        # Training step\n",
    "        losses = train_step(model, images, params, labels, optimizer, LAMBDA_PHYSICS, device)\n",
    "        \n",
    "        for key in losses:\n",
    "            train_losses[key].append(losses[key])\n",
    "    \n",
    "    # Average training losses\n",
    "    avg_train_losses = {key: np.mean(vals) for key, vals in train_losses.items()}\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_losses = {'total': [], 'mse_params': [], 'ce_class': [], 'physics_residual': []}\n",
    "    \n",
    "    for images, params, labels in val_loader:\n",
    "        if isinstance(images, np.ndarray):\n",
    "            images = torch.from_numpy(images).float()\n",
    "        images = images.to(device)\n",
    "        params = params.float().to(device)\n",
    "        labels = labels.long().to(device)\n",
    "        \n",
    "        losses = validate_step(model, images, params, labels, LAMBDA_PHYSICS, device)\n",
    "        \n",
    "        for key in losses:\n",
    "            val_losses[key].append(losses[key])\n",
    "    \n",
    "    # Average validation losses\n",
    "    avg_val_losses = {key: np.mean(vals) for key, vals in val_losses.items()}\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(avg_val_losses['total'])\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    logger.log_training_metrics(\n",
    "        epoch,\n",
    "        avg_train_losses,\n",
    "        avg_val_losses,\n",
    "        current_lr\n",
    "    )\n",
    "    \n",
    "    # Log predictions periodically\n",
    "    if epoch % LOG_INTERVAL == 0:\n",
    "        logger.log_predictions_comparison(model, val_loader, device, epoch, num_samples=6)\n",
    "        logger.log_histograms(model, epoch)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] ({epoch_time:.1f}s) LR: {current_lr:.2e}\")\n",
    "    print(f\"  Train - Loss: {avg_train_losses['total']:.4f} | \" +\n",
    "          f\"MSE: {avg_train_losses['mse_params']:.4f} | \" +\n",
    "          f\"CE: {avg_train_losses['ce_class']:.4f} | \" +\n",
    "          f\"Phys: {avg_train_losses['physics_residual']:.4f}\")\n",
    "    print(f\"  Val   - Loss: {avg_val_losses['total']:.4f} | \" +\n",
    "          f\"MSE: {avg_val_losses['mse_params']:.4f} | \" +\n",
    "          f\"CE: {avg_val_losses['ce_class']:.4f} | \" +\n",
    "          f\"Phys: {avg_val_losses['physics_residual']:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_val_losses['total'] < best_val_loss:\n",
    "        best_val_loss = avg_val_losses['total']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': avg_val_losses['total'],\n",
    "            'train_loss': avg_train_losses['total'],\n",
    "        }, best_model_path)\n",
    "        print(f\"  ✓ Best model saved (val_loss: {best_val_loss:.4f})\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\n⚠ Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    print()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Training completed in {total_time/60:.1f} minutes\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Model saved: {best_model_path}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Log final hyperparameters\n",
    "hparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': 1e-3,\n",
    "    'lambda_physics': LAMBDA_PHYSICS,\n",
    "    'dropout': 0.2,\n",
    "    'augmentation': 'Yes',\n",
    "    'scheduler': 'ReduceLROnPlateau'\n",
    "}\n",
    "metrics = {\n",
    "    'final_val_loss': best_val_loss,\n",
    "    'total_epochs': epoch + 1\n",
    "}\n",
    "logger.log_hyperparameters(hparams, metrics)\n",
    "\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9cad95",
   "metadata": {},
   "source": [
    "## 7. Advanced Training Loop\n",
    "\n",
    "Key improvements:\n",
    "- TensorBoard logging of all metrics\n",
    "- Model checkpointing (save best model)\n",
    "- Early stopping with patience\n",
    "- Gradient clipping for stability\n",
    "- Regular prediction visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a2a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = PhysicsInformedNN(input_size=64, dropout_rate=0.2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Log model architecture to TensorBoard\n",
    "logger.log_model_graph(model, input_size=(1, 1, 64, 64))\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Optimizer with weight decay\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate schedulers (we'll use two strategies)\n",
    "# 1. Reduce on plateau: Reduce LR when validation loss plateaus\n",
    "scheduler_plateau = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# 2. Cosine annealing: Smooth LR decay\n",
    "scheduler_cosine = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    ")\n",
    "\n",
    "# We'll use ReduceLROnPlateau for this demo\n",
    "scheduler = scheduler_plateau\n",
    "\n",
    "print(\"✓ Model and optimizers initialized\")\n",
    "print(f\"  Initial LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "print(f\"  Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db92a3b9",
   "metadata": {},
   "source": [
    "## 6. Initialize Model and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d04af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorBoard logger\n",
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_name = f'pinn_augmented_{timestamp}'\n",
    "\n",
    "logger = PINNLogger(log_dir='../runs', experiment_name=experiment_name)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TensorBoard Logging Enabled!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Experiment: {experiment_name}\")\n",
    "print(f\"\\nTo view logs in real-time, run in terminal:\")\n",
    "print(f\"  tensorboard --logdir=../runs\")\n",
    "print(f\"\\nThen open: http://localhost:6006\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d6a3e5",
   "metadata": {},
   "source": [
    "## 5. Initialize TensorBoard Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da614aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Training set WITH augmentation\n",
    "train_dataset = LensDataset(DATA_FILE, split='train', transform=train_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Validation set WITHOUT augmentation (important!)\n",
    "val_dataset = LensDataset(DATA_FILE, split='val', transform=None)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Training set: {len(train_dataset)} samples (WITH augmentation)\")\n",
    "print(f\"Validation set: {len(val_dataset)} samples (NO augmentation)\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58d1cab",
   "metadata": {},
   "source": [
    "## 4. Load Dataset with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4840f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image\n",
    "DATA_FILE = '../data/processed/lens_training_data.h5'\n",
    "temp_dataset = LensDataset(DATA_FILE, split='train')\n",
    "sample_image, _, _ = temp_dataset[0]\n",
    "\n",
    "# Apply augmentation multiple times\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(sample_image[0], cmap='viridis', origin='lower')\n",
    "axes[0].set_title('Original', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Augmented versions\n",
    "for i in range(1, 8):\n",
    "    aug_image = train_transforms(sample_image.copy())\n",
    "    axes[i].imshow(aug_image[0], cmap='viridis', origin='lower')\n",
    "    axes[i].set_title(f'Augmented {i}', fontsize=12)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Data Augmentation Examples', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Each training iteration applies random augmentations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0302d570",
   "metadata": {},
   "source": [
    "## 3. Visualize Augmentation Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b7d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training transforms with augmentation\n",
    "train_transforms = get_training_transforms(\n",
    "    rotation=True,           # Random 90/180/270 degree rotations\n",
    "    flip=True,               # Random horizontal/vertical flips\n",
    "    brightness=True,         # Random brightness adjustments\n",
    "    noise=True,              # Random Gaussian noise\n",
    "    rotation_p=0.5,         # 50% probability of rotation\n",
    "    flip_p=0.5,             # 50% probability of flip\n",
    "    brightness_p=0.5,       # 50% probability of brightness change\n",
    "    noise_p=0.3,            # 30% probability of noise\n",
    "    brightness_range=(0.8, 1.2),  # ±20% brightness variation\n",
    "    noise_std=0.01          # Small noise for realism\n",
    ")\n",
    "\n",
    "print(\"✓ Data augmentation pipeline created\")\n",
    "print(\"\\nAugmentation Pipeline:\")\n",
    "print(\"  - Random Rotation (90°, 180°, 270°): p=0.5\")\n",
    "print(\"  - Random Flip (H/V): p=0.5\")\n",
    "print(\"  - Random Brightness (0.8-1.2x): p=0.5\")\n",
    "print(\"  - Random Gaussian Noise (σ=0.01): p=0.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a73fe",
   "metadata": {},
   "source": [
    "## 2. Setup Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763fdf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.ml import (\n",
    "    PhysicsInformedNN, get_training_transforms,\n",
    "    PINNLogger\n",
    ")\n",
    "from src.ml.pinn import train_step, validate_step\n",
    "from src.ml.generate_dataset import LensDataset\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "print(\"✓ All modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e01f2e",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02351a",
   "metadata": {},
   "source": [
    "# Phase 5d: Advanced Training with Augmentation & TensorBoard\n",
    "\n",
    "This notebook demonstrates **advanced training features**:\n",
    "\n",
    "## New Features:\n",
    "- ✓ **Data Augmentation**: Rotation, flip, brightness, noise\n",
    "- ✓ **TensorBoard Logging**: Real-time training visualization\n",
    "- ✓ **Learning Rate Scheduling**: Adaptive learning rate\n",
    "- ✓ **Early Stopping**: Prevent overfitting\n",
    "- ✓ **Gradient Clipping**: Training stability\n",
    "- ✓ **Model Checkpointing**: Save best weights\n",
    "\n",
    "## Improvements Over Basic Training:\n",
    "- Better generalization through augmentation\n",
    "- Real-time monitoring with TensorBoard\n",
    "- More robust training procedures\n",
    "- Publication-quality logging\n",
    "\n",
    "Let's implement production-ready training!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
